Matching process requirements (full logic)
0) Inputs

User scope: app_user_id

Model scope: model_id

Model JSON (from match_job.model_json or mdm_models.config_json) contains:

matchFieldCodes (e.g., ["f01","f02","f03"])

per-field:

weight (sum of weights across matchFieldCodes = 1.0)

matchThreshold (field-level gate)

model-level:

matchThreshold (strong match)

possibleThreshold (possible match)

Data tables:

source_input = incoming records for app_user_id

recon_cluster = clustered history for (app_user_id, model_id)

1) Core scoring rules

1.1 Field-level similarity

For any two values (a,b):

compute sim = conservative_similarity(a,b) where:

sim = 1 - lev(a,b) / min(len(a),len(b)) clamped to [0..1]

Threshold=100% shortcut (locked):
If field.matchThreshold is 100% (1.0), then the field only matches on exact equality (non-blank) and Levenshtein is skipped.

Bounded Levenshtein optimization (locked):
When Levenshtein is used, the calculation may stop early once the edit distance exceeds the maximum allowed by field.matchThreshold
(because the field cannot possibly pass its gate). This does not change results.

Missing value rule (locked):
If either value is missing/blank, that field is an automatic 0 match (sim = 0) for that field.

Field passes if: sim >= field.matchThreshold


1.2 Model-level score (weighted gated)

For each match field i:

compute (pass_i, sim_i) via field-level rule

if pass_i == True: contribution = sim_i * weight_i

if pass_i == False: contribution = 0

Total model score:

score = sum(contribution_i)

Weight = 0 means field has no effect on the model score.

1.3 Pair classification by score

If score >= model.matchThreshold → strong match

Else if possibleThreshold <= score < matchThreshold → possible match

Else → no match


2) Candidate generation (blocking)

Goal: avoid O(N²) scans; restrict candidate comparisons.

DB indexing (locked):
To keep candidate queries fast, the database must have indexes supporting scope lookups and ordering, including:
- recon_cluster(app_user_id, model_id, cluster_id, source_name, source_id)
- recon_cluster(app_user_id, model_id, source_name, source_id)
- source_input(app_user_id, source_name, source_id)


2.1 Blocking fields selection

Use matchFieldCodes sorted by weight desc

Use top 2 fields by default; if missing values, fall back to next weighted fields.

2.2 Adaptive prefix blocking

Normalize values: lower, trim, strip punctuation, collapse whitespace.

Build block keys using prefixes of the top weighted fields.

Tighten progressively until candidate count ≤ target (e.g., 1000):

1+1, then 2+1, then 2+2, then 3+2, then keep increasing.

Candidate set is produced for each record using this adaptive logic.

If no blockers available, fall back to a capped scan (limited size).

3) Two operating modes
Mode A: Initial run (bootstrap) — when recon_cluster is empty for (app_user_id, model_id)

Objective: cluster the incoming batch against itself.

A1) Build strong-match clusters (matchThreshold)

For each record r in the batch:

Generate candidate records from the batch using blocking.

For each candidate c:

compute model score(r,c)

if score >= matchThreshold: create a strong match link between r and c.

Build clusters as connected components of strong links (transitive closure).

A2) Assign cluster_id to strong clusters

Each strong cluster gets one cluster_id (UUID).

All records in those clusters:

cluster_id = cluster.cluster_id

match_status = "match"

A3) “Find home” for remaining records (possibleThreshold)

For any record not in a strong cluster:

Find its best possible cluster candidate using model score:

consider possible links to any record already assigned to a cluster (or to any record; cluster-level pick resolves later)

choose the candidate cluster with highest score

tie-breaker: smallest cluster_id

If best possible score ≥ possibleThreshold:

assign cluster_id = best_candidate_cluster_id

set match_status = "exception"

write an exception row (record → candidate cluster + score)

If no possible candidate exists:

create a new single-record cluster:

new cluster_id

match_status = "no_match"

A4) Write outputs

Insert all records into recon_cluster with:

cluster_id

match_status in (match, exception, no_match)

original fields source_name/source_id/f01..f20/audit

Upsert cluster_map for (source_name, source_id) → cluster_id.

Mode B: Incremental run — when recon_cluster already has rows

Objective: assign new incoming records into existing clusters, or create new ones.

B1) Identify new records

Process only source_input rows that do not exist in recon_cluster for the same (app_user_id, model_id, source_name, source_id).

B2) For each new record, find best home

Generate candidate clusters from recon_cluster using blocking.

Compute model score against candidates.

Pick best candidate cluster:

max score

tie-breaker: smallest cluster_id

B3) Assign + classify

If best score ≥ matchThreshold:

assign that cluster_id

match_status = "match"

Else if best score ≥ possibleThreshold:

assign that cluster_id

match_status = "exception"

write exception row

Else:

create new cluster_id

match_status = "no_match"

B4) Persist

Insert into recon_cluster

Upsert cluster_map

Insert exception rows when status = exception

4) Tie-breaking

When multiple candidate clusters have equal best score:

choose the one with smallest cluster_id.

5) Non-negotiable outputs

Every record gets a cluster_id:

either from a strong cluster, best possible home, or a new single-record cluster.

Every record has match_status:

match, exception, or no_match.





##########  How the dynamic candidate selection works ##########
Goal

For each incoming record, generate a candidate set from the matching scope that is big enough to not miss matches but small enough to score fast: 250–500 candidates whenever possible. (This replaces the old “target=1000” style logic.)

matching process

Inputs

Match fields: ordered list of matchFieldCodes + each field’s weight (ties keep the model order).

matching process

matching process

Scope:

Incremental mode: candidates come from recon_cluster for (app_user_id, model_id).

matching process

matching process

Bootstrap mode: candidates come from the incoming batch (same logic, different pool).

matching process

Candidate band: MIN=250, MAX=500 (locked).

Max prefix length per field: bounded by the normalized value length and a system max (implementation detail).

Normalization (must be identical everywhere)

Before building prefixes, normalize all field values:

lower, trim, strip punctuation, collapse whitespace.

matching process

Core rule: how prefixes are built (this is the part we’re “locking”)

Maintain prefix_len[field] starting at 0 for all usable fields.

Field selection order

Always start with the highest weight field.

If multiple fields have the same weight, resolve ties by model order.

matching process

“Proportional to weight” increment rule (deterministic)

You add one character at a time. Each step you choose one field to increment by 1.

To choose which field to increment next:

For each usable field, compute its next-step priority as:
priority = weight / (current_prefix_len + 1)

Increment the field with the highest priority.

Tie-breakers (in order):

higher weight

earlier in model order

This guarantees:

You start with the highest-weight field.

You keep incrementing it first until it stops being “worth more” than starting/expanding the next field.

Over time, fields get prefix growth in a way that’s proportional to weights (your 70/30 example naturally becomes ~2:1 early on).

Block key definition

At any step, your blocking filter is:

An AND of all fields where prefix_len > 0:

candidate_field_value starts with record_field_value_prefix

Stop conditions (range 250–500)

After each increment step, query the scope and observe candidate count N:

If 250 ≤ N ≤ 500 → STOP and return these candidates.

If N > 500 → continue tightening (another increment step).

If N < 250 → you likely over-tightened; see “backoff/cap” rules below.

Backoff + cap rules (prevents “too many → zero” failures)

You must track:

the last candidate set with N > 0

the last candidate set with N > 500 (the last “too many” set)

Rules:

If a tighter step produces N = 0, you must revert to the last non-empty set (never return empty due to over-tightening).

If you drop from >500 to <250 (overshoot), you should return a deterministic capped subset of 500 from the last “>500” set (this keeps you in the 250–500 band and avoids starving the scorer).

If the scope genuinely has <250 total records, return all of them (can’t manufacture candidates).

If all match fields are blank/unusable for this record, fall back to a capped scan of 500 from scope.

matching process

Determinism (so runs are stable)

When you need to cap a too-large set to 500, the selection must be deterministic (same input → same 500).
Example deterministic ordering (implementation detail): fixed sort by stable keys like (cluster_id, source_name, source_id).